}
# Select best parameters
best_rf_math <- rf_results_math[which.min(rf_results_math$CV_MSPE), ]
best_rf_por  <- rf_results_por[which.min(rf_results_por$CV_MSPE), ]
# Train final models with best hyperparameters
final_model_math <- randomForest(formula_math, data = train_data,
mtry = best_rf_math$mtry, ntree = best_rf_math$ntree)
final_model_por <- randomForest(formula_por, data = train_data,
mtry = best_rf_por$mtry, ntree = best_rf_por$ntree)
# Predict on test set
final_preds_math <- predict(final_model_math, newdata = test_data)
final_preds_por  <- predict(final_model_por, newdata = test_data)
# Compute final MSPE on test data
final_mspe_math <- mse(test_data$G3_math, final_preds_math)
final_mspe_por  <- mse(test_data$G3_por, final_preds_por)
# Output
print(best_rf_math)
cat("Final MSPE on test (math):", final_mspe_math, "\n")
print(best_rf_por)
cat("Final MSPE on test (portuguese):", final_mspe_por, "\n")
print(mspe_xgb_math)
print(mspe_xgb_por)
xgb_grid <- expand.grid(
nrounds = c(50, 100, 150),
max_depth = c(3, 5, 7),
eta = c(0.01, 0.1, 0.3)
)
xgb_model_math <- train(formula_math, data = train_data, method = "xgbTree", trControl = ctrl, tuneGrid = xgb_grid)
xgb_model_por <- train(formula_por, data = train_data, method = "xgbTree", trControl = ctrl, tuneGrid = xgb_grid)
xgb_preds_math <- predict(xgb_model_math, test_data)
xgb_preds_por <- predict(xgb_model_por, test_data)
mspe_xgb_math <- mse(test_data$G3_math, xgb_preds_math)
mspe_xgb_por <- mse(test_data$G3_por, xgb_preds_por)
print(mspe_xgb_math)
print(mspe_xgb_por)
xgb_grid <- expand.grid(
nrounds = c(50, 100, 150),
max_depth = c(3, 5, 7),
eta = c(0.01, 0.1, 0.3)
)
xgb_model_math <- train(formula_math, data = train_data, method = "xgbTree", trControl = ctrl, tuneGrid = xgb_grid)
# Model 2: XGBoost
ctrl <- trainControl(method = "cv", number = 5)
xgb_grid <- expand.grid(
nrounds = c(50, 100, 150),
max_depth = c(3, 5, 7),
eta = c(0.01, 0.1, 0.3),
gamma = c(0.5, 1, 1.5, 2, 5),
colsample_bytree = c(0.6, 0.8, 1.0),
min_child_weight = c(1,5,10),
subsample = c(0.6, 0.8, 1.0)
)
xgb_model_math <- train(formula_math, data = train_data, method = "xgbTree", trControl = ctrl, tuneGrid = xgb_grid)
xgb_model_por <- train(formula_por, data = train_data, method = "xgbTree", trControl = ctrl, tuneGrid = xgb_grid)
xgb_preds_math <- predict(xgb_model_math, test_data)
xgb_preds_por <- predict(xgb_model_por, test_data)
mspe_xgb_math <- mse(test_data$G3_math, xgb_preds_math)
mspe_xgb_por <- mse(test_data$G3_por, xgb_preds_por)
print(mspe_xgb_math)
print(mspe_xgb_por)
# Model 3: Random Forest
library(randomForest)
library(caret)
library(Metrics)  # for mse()
# Define hyperparameters
rf_mtry   <- c(2, 4, 6, 8, 10)
rf_ntrees <- c(100, 300, 500, 1000)
# Grid of parameters
param_grid <- expand.grid(mtry = rf_mtry, ntree = rf_ntrees)
# Set up results data.frames
rf_results_math <- param_grid
rf_results_math$CV_MSPE <- NA_real_
rf_results_por <- param_grid
rf_results_por$CV_MSPE <- NA_real_
# Define 5-fold CV
set.seed(123)
folds <- createFolds(train_data$G3_math, k = 5, returnTrain = TRUE)
# Loop over parameter grid
for (i in seq_len(nrow(param_grid))) {
m  <- param_grid$mtry[i]
nt <- param_grid$ntree[i]
mspe_math_folds <- c()
mspe_por_folds  <- c()
for (fold in folds) {
cv_train <- train_data[fold, ]
cv_val   <- train_data[-fold, ]
# Train model for math
model_math <- randomForest(formula_math, data = cv_train, mtry = m, ntree = nt)
preds_math <- predict(model_math, newdata = cv_val)
mspe_math_folds <- c(mspe_math_folds, mse(cv_val$G3_math, preds_math))
# Train model for portuguese
model_por <- randomForest(formula_por, data = cv_train, mtry = m, ntree = nt)
preds_por <- predict(model_por, newdata = cv_val)
mspe_por_folds <- c(mspe_por_folds, mse(cv_val$G3_por, preds_por))
}
# Store average CV MSPE
rf_results_math$CV_MSPE[i] <- mean(mspe_math_folds)
rf_results_por$CV_MSPE[i]  <- mean(mspe_por_folds)
}
# Select best parameters
best_rf_math <- rf_results_math[which.min(rf_results_math$CV_MSPE), ]
best_rf_por  <- rf_results_por[which.min(rf_results_por$CV_MSPE), ]
# Train final models with best hyperparameters
final_model_math <- randomForest(formula_math, data = train_data,
mtry = best_rf_math$mtry, ntree = best_rf_math$ntree)
final_model_por <- randomForest(formula_por, data = train_data,
mtry = best_rf_por$mtry, ntree = best_rf_por$ntree)
# Predict on test set
final_preds_math <- predict(final_model_math, newdata = test_data)
final_preds_por  <- predict(final_model_por, newdata = test_data)
# Compute final MSPE on test data
final_mspe_math <- mse(test_data$G3_math, final_preds_math)
final_mspe_por  <- mse(test_data$G3_por, final_preds_por)
# Output
print(best_rf_math)
cat("Final MSPE on test (math):", final_mspe_math, "\n")
print(best_rf_por)
cat("Final MSPE on test (portuguese):", final_mspe_por, "\n")
# Model 4: Feedforward Neural Network (nnet)
# Define hyperparameters
rf_mtry   <- c(2, 4, 6, 8, 10)
rf_ntrees <- c(100, 300, 500, 1000)
# Grid of parameters
param_grid <- expand.grid(mtry = rf_mtry, ntree = rf_ntrees)
# Set up results data.frames
rf_results_math <- param_grid
rf_results_math$CV_MSPE <- NA_real_
rf_results_por <- param_grid
rf_results_por$CV_MSPE <- NA_real_
# Define 5-fold CV
set.seed(42)
folds <- createFolds(train_data$G3_math, k = 5, returnTrain = TRUE)
# Loop over parameter grid
for (i in seq_len(nrow(param_grid))) {
m  <- param_grid$mtry[i]
nt <- param_grid$ntree[i]
mspe_math_folds <- c()
mspe_por_folds  <- c()
for (fold in folds) {
cv_train <- train_data[fold, ]
cv_val   <- train_data[-fold, ]
# Train model for math
model_math <- randomForest(formula_math, data = cv_train, mtry = m, ntree = nt)
preds_math <- predict(model_math, newdata = cv_val)
mspe_math_folds <- c(mspe_math_folds, mse(cv_val$G3_math, preds_math))
# Train model for portuguese
model_por <- randomForest(formula_por, data = cv_train, mtry = m, ntree = nt)
preds_por <- predict(model_por, newdata = cv_val)
mspe_por_folds <- c(mspe_por_folds, mse(cv_val$G3_por, preds_por))
}
# Store average CV MSPE
rf_results_math$CV_MSPE[i] <- mean(mspe_math_folds)
rf_results_por$CV_MSPE[i]  <- mean(mspe_por_folds)
}
# Select best parameters
best_rf_math <- rf_results_math[which.min(rf_results_math$CV_MSPE), ]
best_rf_por  <- rf_results_por[which.min(rf_results_por$CV_MSPE), ]
# Train final models with best hyperparameters
final_model_math <- randomForest(formula_math, data = train_data,
mtry = best_rf_math$mtry, ntree = best_rf_math$ntree)
final_model_por <- randomForest(formula_por, data = train_data,
mtry = best_rf_por$mtry, ntree = best_rf_por$ntree)
# Predict on test set
final_preds_math <- predict(final_model_math, newdata = test_data)
final_preds_por  <- predict(final_model_por, newdata = test_data)
# Compute final MSPE on test data
final_mspe_math <- mse(test_data$G3_math, final_preds_math)
final_mspe_por  <- mse(test_data$G3_por, final_preds_por)
# Output
print(best_rf_math)
cat("Final MSPE on test (math):", final_mspe_math, "\n")
print(best_rf_por)
cat("Final MSPE on test (portuguese):", final_mspe_por, "\n")
# Model 4: Feedforward Neural Network (nnet)
final_mspe_math
final_mspe_por
# Define hyperparameters
rf_mtry   <- c(2, 4, 6, 8, 10)
rf_ntrees <- c(100, 300, 500, 1000)
# Grid of parameters
param_grid <- expand.grid(mtry = rf_mtry, ntree = rf_ntrees)
# Set up results data.frames
rf_results_math <- param_grid
rf_results_math$CV_MSPE <- NA_real_
rf_results_por <- param_grid
rf_results_por$CV_MSPE <- NA_real_
# Define 5-fold CV
folds <- createFolds(train_data$G3_math, k = 5, returnTrain = TRUE)
# Loop over parameter grid
for (i in seq_len(nrow(param_grid))) {
m  <- param_grid$mtry[i]
nt <- param_grid$ntree[i]
mspe_math_folds <- c()
mspe_por_folds  <- c()
for (fold in folds) {
cv_train <- train_data[fold, ]
cv_val   <- train_data[-fold, ]
# Train model for math
model_math <- randomForest(formula_math, data = cv_train, mtry = m, ntree = nt)
preds_math <- predict(model_math, newdata = cv_val)
mspe_math_folds <- c(mspe_math_folds, mse(cv_val$G3_math, preds_math))
# Train model for portuguese
model_por <- randomForest(formula_por, data = cv_train, mtry = m, ntree = nt)
preds_por <- predict(model_por, newdata = cv_val)
mspe_por_folds <- c(mspe_por_folds, mse(cv_val$G3_por, preds_por))
}
# Store average CV MSPE
rf_results_math$CV_MSPE[i] <- mean(mspe_math_folds)
rf_results_por$CV_MSPE[i]  <- mean(mspe_por_folds)
}
# Select best parameters
best_rf_math <- rf_results_math[which.min(rf_results_math$CV_MSPE), ]
best_rf_por  <- rf_results_por[which.min(rf_results_por$CV_MSPE), ]
# Train final models with best hyperparameters
final_model_math <- randomForest(formula_math, data = train_data,
mtry = best_rf_math$mtry, ntree = best_rf_math$ntree)
final_model_por <- randomForest(formula_por, data = train_data,
mtry = best_rf_por$mtry, ntree = best_rf_por$ntree)
# Predict on test set
final_preds_math <- predict(final_model_math, newdata = test_data)
final_preds_por  <- predict(final_model_por, newdata = test_data)
# Compute final MSPE on test data
final_mspe_math <- mse(test_data$G3_math, final_preds_math)
final_mspe_por  <- mse(test_data$G3_por, final_preds_por)
# Output
print(best_rf_math)
cat("Final MSPE on test (math):", final_mspe_math, "\n")
print(best_rf_por)
cat("Final MSPE on test (portuguese):", final_mspe_por, "\n")
# Subset to only necessary columns
cols_to_keep <- c(predictors, "G3_math", "G3_por")
train_data <- train_data[, cols_to_keep]
test_data  <- test_data[, cols_to_keep]
# Normalize numeric features
scale_features <- function(df) {
df_scaled <- df
num_cols <- sapply(df, is.numeric)
df_scaled[num_cols] <- scale(df[num_cols])
return(df_scaled)
}
train_data_scaled <- scale_features(train_data)
test_data_scaled  <- scale_features(test_data)
# Extract inputs and outputs
x_train <- as.matrix(train_data_scaled[, predictors])
y_train <- as.matrix(train_data_scaled[, c("G3_math", "G3_por")])
x_test  <- as.matrix(test_data_scaled[, predictors])
y_test  <- as.matrix(test_data_scaled[, c("G3_math", "G3_por")])
# Define hyperparameters to tune
hidden_layers <- c(1, 2)
hidden_nodes  <- c(16, 32, 64)
batch_sizes   <- c(32, 64)
epochs_list   <- c(50, 100)
# Parameter grid
param_grid <- expand.grid(layers = hidden_layers,
nodes = hidden_nodes,
batch_size = batch_sizes,
epochs = epochs_list)
# Result storage
fnn_results <- param_grid
fnn_results$CV_MSPE_math <- NA_real_
fnn_results$CV_MSPE_por  <- NA_real_
# 5-fold cross-validation
folds <- createFolds(y_train[, 1], k = 5, returnTrain = TRUE)
# Loop through grid
for (i in seq_len(nrow(param_grid))) {
layer_n <- param_grid$layers[i]
node_n  <- param_grid$nodes[i]
bs      <- param_grid$batch_size[i]
ep      <- param_grid$epochs[i]
mspe_math_folds <- c()
mspe_por_folds  <- c()
for (fold in folds) {
x_fold_train <- x_train[fold, ]
y_fold_train <- y_train[fold, ]
x_fold_val   <- x_train[-fold, ]
y_fold_val   <- y_train[-fold, ]
# Define model
model <- keras_model_sequential()
model %>% layer_dense(units = node_n, activation = 'relu', input_shape = ncol(x_train))
if (layer_n == 2) {
model %>% layer_dense(units = node_n, activation = 'relu')
}
model %>% layer_dense(units = 2)  # Two output nodes
model %>% compile(
loss = 'mse',
optimizer = optimizer_adam(),
metrics = list('mse')
)
# Train model
model %>% fit(
x = x_fold_train, y = y_fold_train,
epochs = ep, batch_size = bs,
verbose = 0
)
# Predict
preds <- model %>% predict(x_fold_val)
# Compute MSPE for both outputs
mspe_math_folds <- c(mspe_math_folds, mean((y_fold_val[, 1] - preds[, 1])^2))
mspe_por_folds  <- c(mspe_por_folds,  mean((y_fold_val[, 2] - preds[, 2])^2))
}
# Store average CV errors
fnn_results$CV_MSPE_math[i] <- mean(mspe_math_folds)
fnn_results$CV_MSPE_por[i]  <- mean(mspe_por_folds)
}
# Model 4: Feedforward Neural Network (nnet)
library(keras)
library(tensorflow)
# Subset to only necessary columns
cols_to_keep <- c(predictors, "G3_math", "G3_por")
train_data <- train_data[, cols_to_keep]
test_data  <- test_data[, cols_to_keep]
# Normalize numeric features
scale_features <- function(df) {
df_scaled <- df
num_cols <- sapply(df, is.numeric)
df_scaled[num_cols] <- scale(df[num_cols])
return(df_scaled)
}
train_data_scaled <- scale_features(train_data)
test_data_scaled  <- scale_features(test_data)
# Extract inputs and outputs
x_train <- as.matrix(train_data_scaled[, predictors])
y_train <- as.matrix(train_data_scaled[, c("G3_math", "G3_por")])
x_test  <- as.matrix(test_data_scaled[, predictors])
y_test  <- as.matrix(test_data_scaled[, c("G3_math", "G3_por")])
# Define hyperparameters to tune
hidden_layers <- c(1, 2)
hidden_nodes  <- c(16, 32, 64)
batch_sizes   <- c(32, 64)
epochs_list   <- c(50, 100)
# Parameter grid
param_grid <- expand.grid(layers = hidden_layers,
nodes = hidden_nodes,
batch_size = batch_sizes,
epochs = epochs_list)
# Result storage
fnn_results <- param_grid
fnn_results$CV_MSPE_math <- NA_real_
fnn_results$CV_MSPE_por  <- NA_real_
# 5-fold cross-validation
folds <- createFolds(y_train[, 1], k = 5, returnTrain = TRUE)
# Loop through grid
for (i in seq_len(nrow(param_grid))) {
layer_n <- param_grid$layers[i]
node_n  <- param_grid$nodes[i]
bs      <- param_grid$batch_size[i]
ep      <- param_grid$epochs[i]
mspe_math_folds <- c()
mspe_por_folds  <- c()
for (fold in folds) {
x_fold_train <- x_train[fold, ]
y_fold_train <- y_train[fold, ]
x_fold_val   <- x_train[-fold, ]
y_fold_val   <- y_train[-fold, ]
# Define model
model <- keras_model_sequential()
model %>% layer_dense(units = node_n, activation = 'relu', input_shape = ncol(x_train))
if (layer_n == 2) {
model %>% layer_dense(units = node_n, activation = 'relu')
}
model %>% layer_dense(units = 2)  # Two output nodes
model %>% compile(
loss = 'mse',
optimizer = optimizer_adam(),
metrics = list('mse')
)
# Train model
model %>% fit(
x = x_fold_train, y = y_fold_train,
epochs = ep, batch_size = bs,
verbose = 0
)
# Predict
preds <- model %>% predict(x_fold_val)
# Compute MSPE for both outputs
mspe_math_folds <- c(mspe_math_folds, mean((y_fold_val[, 1] - preds[, 1])^2))
mspe_por_folds  <- c(mspe_por_folds,  mean((y_fold_val[, 2] - preds[, 2])^2))
}
# Store average CV errors
fnn_results$CV_MSPE_math[i] <- mean(mspe_math_folds)
fnn_results$CV_MSPE_por[i]  <- mean(mspe_por_folds)
}
?nnet
# Model 4: 1 hidden layer Feedforward Neural Network (nnet)
nnet_grid <- expand.grid(
size = c(1, 3, 5, 7, 10, 15),           # number of hidden units
decay = c(0, 0.0001, 0.001, 0.01, 0.1), # regularization strength
maxit = c(200, 500, 1000),              # training iterations
linout = TRUE                           # TRUE for regression
)
nnet_model_math <- train(cbind(G3_math, G3_por) ~ ., data = train_data[, c(target_vars, predictors)], method = "nnet", linout = TRUE, trace = FALSE,
trControl = ctrl, tuneGrid = nnet_grid)
nnet(cbind(G3_math, G3_por) ~ ., data = train_data[, c(target_vars, predictors)], method = "nnet", linout = TRUE, trace = FALSE,)
nnet(cbind(G3_math, G3_por) ~ ., data = train_data[, c(target_vars, predictors)], method = "nnet", linout = TRUE, trace = FALSE, size=10)
model <- nnet(cbind(G3_math, G3_por) ~ ., data = train_data[, c(target_vars, predictors)], method = "nnet", linout = TRUE, trace = FALSE, size=10)
model
View(model)
# Model 4: 1 hidden layer Feedforward Neural Network (nnet)
ctrl <- trainControl(method = "cv", number = 5)
# Define tuning grid
nnet_grid <- expand.grid(
size = c(1, 3, 5, 7, 10, 15),           # number of hidden units
decay = c(0, 0.0001, 0.001, 0.01, 0.1)  # regularization
)
# Train separate model for G3_math
nnet_model_math <- train(
G3_math ~ .,
data = train_data[, c("G3_math", predictors)],
method = "nnet",
linout = TRUE,
trace = FALSE,
maxit = 500,  # default, or you can tune externally
tuneGrid = nnet_grid,
trControl = ctrl
)
# Train separate model for G3_por
nnet_model_por <- train(
G3_por ~ .,
data = train_data[, c("G3_por", predictors)],
method = "nnet",
linout = TRUE,
trace = FALSE,
maxit = 500,
tuneGrid = nnet_grid,
trControl = ctrl
)
library(caret)
library(nnet)
library(Metrics)
ctrl <- trainControl(method = "cv", number = 5)
# Define tuning grid
nnet_grid <- expand.grid(
size = c(1, 3, 5, 7, 10, 15),           # number of hidden units
decay = c(0, 0.0001, 0.001, 0.01, 0.1)  # regularization
)
# Train separate model for G3_math
nnet_model_math <- train(
G3_math ~ .,
data = train_data[, c("G3_math", predictors)],
method = "nnet",
linout = TRUE,
trace = FALSE,
maxit = 500,  # default, or you can tune externally
tuneGrid = nnet_grid,
trControl = ctrl
)
nnet_grid <- expand.grid(size = c(5, 10, 15), decay = c(0, 0.1, 0.5, 1))
nnet_model_math <- train(formula_math, data = train_data, method = "nnet", linout = TRUE, trace = FALSE,
trControl = ctrl, tuneGrid = nnet_grid)
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
# Define formula
predictors <- c("famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "famrel", "famsup",
"guardian", "internet", "school", "sex", "age", "address", "traveltime",
"freetime", "nursery", "schoolsup", "activities", "higher", "romantic",
"goout")
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
formula_por <- as.formula(paste("G3_por ~", paste(predictors, collapse = " + ")))
predictors <- c("famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "famrel", "famsup",
"guardian", "internet", "school", "sex", "age", "address", "traveltime",
"freetime", "nursery", "schoolsup", "activities", "higher", "romantic",
"goout")
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
formula_por <- as.formula(paste("G3_por ~", paste(predictors, collapse = " + ")))
ctrl <- trainControl(method = "cv", number = 5)
nnet_grid <- expand.grid(size = c(5, 10, 15), decay = c(0, 0.1, 0.5, 1))
nnet_model_math <- train(formula_math, data = train_data, method = "nnet", linout = TRUE, trace = FALSE,
trControl = ctrl, tuneGrid = nnet_grid)
mspe_nnet_math
mspe_nnet_por
formula_math
# Predictive performance comparison
# Load required libraries
library(caret)
library(randomForest)
library(xgboost)
library(nnet)
library(MASS)
library(randomForest)
library(xgboost)
library(nnet)
library(MASS)
library(dplyr)
library(Metrics)
set.seed(42)
# Define target variables
target_vars <- c("G3_math", "G3_por")
data <- read.csv("data/student-merge.csv", header = TRUE)
data_nodrop <- data %>%
filter(G3_math != 0 & G3_por != 0 & G1_math != 0 & G1_por != 0 & G2_math != 0 & G2_por != 0)
# Define target variables
target_vars <- c("G3_math", "G3_por")
# Split into train and test (80/20)
train_index <- createDataPartition(data_nodrop$G3_math, p = 0.8, list = FALSE)
train_data <- data_nodrop[train_index, ]
test_data <- data_nodrop[-train_index, ]
# Define formula
predictors <- c("famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "famrel", "famsup",
"guardian", "internet", "school", "sex", "age", "address", "traveltime",
"freetime", "nursery", "schoolsup", "activities", "higher", "romantic",
"goout")
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
formula_por <- as.formula(paste("G3_por ~", paste(predictors, collapse = " + ")))
predictors <- c("famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "famrel", "famsup",
"guardian", "internet", "school", "sex", "age", "address", "traveltime",
"freetime", "nursery", "schoolsup", "activities", "higher", "romantic",
"goout")
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
formula_por <- as.formula(paste("G3_por ~", paste(predictors, collapse = " + ")))
ctrl <- trainControl(method = "cv", number = 5)
nnet_grid <- expand.grid(size = c(5, 10, 15), decay = c(0, 0.1, 0.5, 1))
nnet_model_math <- train(formula_math, data = train_data, method = "nnet", linout = TRUE, trace = FALSE,
trControl = ctrl, tuneGrid = nnet_grid)
