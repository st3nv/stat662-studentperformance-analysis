library(caret)
library(randomForest)
library(xgboost)
library(nnet)
library(MASS)
library(dplyr)
library(Metrics)

set.seed(42)

data <- read.csv("data/student-merge.csv", header = TRUE)
data_nodrop <- data %>% 
  filter(G3_math != 0 & G3_por != 0 & G1_math != 0 & G1_por != 0 & G2_math != 0 & G2_por != 0) 

# Define target variables
target_vars <- c("G3_math", "G3_por")

# Split into train and test (80/20)
train_index <- createDataPartition(data_nodrop$G3_math, p = 0.8, list = FALSE)
train_data <- data_nodrop[train_index, ]
test_data <- data_nodrop[-train_index, ]

# Define formula
predictors <- c("famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "famrel", "famsup",
                "guardian", "internet", "school", "sex", "age", "address", "traveltime",
                "freetime", "nursery", "schoolsup", "activities", "higher", "romantic",
                "goout")
formula_math <- as.formula(paste("G3_math ~", paste(predictors, collapse = " + ")))
formula_por <- as.formula(paste("G3_por ~", paste(predictors, collapse = " + ")))

# Model 4: 1 hidden layer Feedforward Neural Network (nnet)
ctrl <- trainControl(method = "cv", number = 5)

nnet_grid <- expand.grid(
  size = c(1, 3, 5, 7, 10, 15, 20),           # number of hidden units
  decay = c(0, 0.0001, 0.001, 0.01, 0.1), # regularization strength
  maxit = c(100, 200, 500)              # training iterations
)

nnet_model_math <- train(formula_math, data = train_data, method = "nnet", linout = TRUE, trace = FALSE,
                         trControl = ctrl, tuneGrid = nnet_grid)
nnet_model_por <- train(formula_por, data = train_data, method = "nnet", linout = TRUE, trace = FALSE,
                        trControl = ctrl, tuneGrid = nnet_grid)
nnet_preds_math <- predict(nnet_model_math, test_data)
nnet_preds_por <- predict(nnet_model_por, test_data)
mspe_nnet_math <- mse(test_data$G3_math, nnet_preds_math)
mspe_nnet_por <- mse(test_data$G3_por, nnet_preds_por)
print(mspe_nnet_math)
print(mspe_nnet_por)
